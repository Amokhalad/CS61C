\documentclass[12pt]{article}
\usepackage[lecture]{preamble}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%          Code Syntax Highlighting
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{fullpage}
\usepackage{listings}
\usepackage{color}
\usepackage{multirow}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\lstset{
%   backgroundcolor=\color{},   % choose the background color
  basicstyle=\ttfamily,        % size of fonts used for the code
  breaklines=true,                 % automatic line breaking only at whitespace
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  escapeinside={\%*}{*},          % if you want to add LaTeX within your code
  keywordstyle={\bfseries \color{blue}},       % keyword style
  stringstyle=\color{mymauve},     % string literal style
  language=C,
  numbers=none % Remove line numbers
}
% End

% package for images
\usepackage{graphicx}
\graphicspath{ {./images/} }




\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%          Lecture 1: Into, Number Rep
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture[06/20/2023]{Intro, Number Rep}
The value of the \emph{i}th digit \emph{d} in any number base is $d \times \text{Base}^i$ where \emph{i} starts 0 and increases from right to left.

\subsection*{Signed Magnitude}
Similar to unsigned numbers but the most significant bit (first bit) represents if the number is positive or negative, this bit is called the \emph{sign bit}.

The issue with this representation is that there are two representations for the number 0, and operations are slow since there is additional work required to handel the sign bit.
\subsection*{Two's Complement}
The convention to represent signed numbers is called \textbf{Two's Complement}. The left most bit is the sign bit, and $1111 \; ... \; 1111_{\text{two}}$ is the most negative number. The advantage twos complement has over \emph{sign and magnitude} representation is that there is only one zero.

\subsubsection*{Negation Shortcut}
Simply flip bits, then add 1 to the result.
This reason this shortcut works is because the sum of a number and its inverted representation must be $1111 \; ... \; 1111_{\text{two}}$ \emph{(there is no carries)}.

Now since $x + \overline{x} = -1 $, we have $\overline{x} + 1 = -x$

\subsubsection*{Sign Extension Shortcut}
\begin{itemize}
    \item for positive 16 $\rightarrow$ 32 bit binary numbers, just add 16 zeros in the most significant bit
    \item For negative 16 $\rightarrow$ 32 bit binary numbers, copy the sign bit (which is 1) 16 times, placing it on the left of the number.
\end{itemize}

This works because positive numbers have an infinite number of leading zeros and negative numbers have an infinite number of leading ones.

\subsection*{One's Complement}
A representation in which the negative of a none's complement is found by inverting each bit. So now $11\dots110_{\text{two}}$ is equal to -1. This representation is similar to twos complement but has two 0s
\subsection*{Biased Notation}
A notation that represents the most negative value by $00 \ldots000_{\text{two}}$ and the most positive value by $11 \ldots 111_{\text{two}}$, with zero typically having the value $10 \ldots000_{\text{two}}$, thereby biasing the number such that the number plus the bias has a nonnegative representation.

In simpler words, bias notation is like unsigned representation but has a shift (a bias), shifting the range of values on the unsigned number line to the left, allowing representation of negative numbers.

With this new system, to interpret a binary string in bias notation, we evaluate the number as if it was unsigned, then add the bias (the bias is usually negative). Note we do this because we shift the numbers to the left by a bias.

\begin{example}[Interpreting a Stored Binary]
    Assume we have a $-127$ bias with an 8-bit number.

    To read $0b0000 \; 1001$, we treat it as if it was unsigned, which gives us 9. Then we add the bias to get our value $-118$:

    $9 + (-127) = -118$
\end{example}

To store a decimal number as a binary string in bias notation, we first subtract the bias (bias is negative so basically add) then store the resulting number as an unsigned binary.

\begin{lemma}
    Subtracting a bias will never give us a negative number.

    This is because the range of numbers is from $[-B, \;  2^n - 1 - B]$, where B is the magnitude of the bias. So a number $x$, will always be greater than or equal to $-B$, and obviously $x - (-B) \ge 0$.
\end{lemma}


\section*{Questions}
\begin{itemize}
    \item is right shift and left shift division and multiplication by 2 respectively?
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%          Lecture 2: C Basics
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture[06/21/2023]{C Basics}
\section*{Chapter 0: Introduction}

The arguments to functions are passed by copying the value of the argument, and it is impossible for the called function to change the actual argument in the caller. When desired to achieve "call by reference," a pointer may be passed explicitly, and the function may change the object to which the pointer points.

Array name are passed as the location of the array origin, so array arguments are effectively call by reference.

C is not a strongly-typed language. It is relatively permissive about data conversion, although it will not automatically convert data types with the wild abandon of PL/I




\section*{Chapter 2: Types, Operators, and Expressions}

A \textbf{string constant} is a sequence of zero or more characters surrounded by double quotes. Underneath, a string is an array whos elements are single characters. The compiler automatically places the null character \lstinline|\0| at the end of each string, so the program can conveniently find the end.

\subsubsection*{Bitwise Operations}
In C, a leading 0 on an int constant implies \emph{\textbf{octal}}. A leading \lstinline|0x| indicates \emph{\textbf{hexa-decimal}}.

The bitwise AND operator \lstinline|&| is used to turn off bits.

The bitwise OR operator \lstinline||| is used to turn on bits.

The one's operator \lstinline|~| (one's complement) is used to flip bits.



\subsection{Memory Model Review}
Memory works very similar to an array. You can think of most version of memory you work with conceptually to be one very long array,

\subsubsection{The Stack}
The stack is memeory that is automatically allocated and freed by the system, and grows from top-down.
\textbf{What is the stack used for?}
\begin{itemize}
    \item Anything considered "Temporary"
    \item This includes: local variables, local constants, arguments to functions, local information about a function call.
\end{itemize}

\textbf{Stack Frames, function calls}
\begin{itemize}
    \item Everyime a function is called, a new "stack frame" is allocated on the stack
    \item Stack Frame Includes:
          \begin{itemize}
              \item return "instruction" address (who called me?), arguments, and space for other local variables
          \end{itemize}
    \item When function ends, stack frame is tossed off the stack; automatically frees memory for future stack frames.
\end{itemize}


\subsubsection{The Heap}





\section*{Questions}
\begin{itemize}
    \item What is an example of local information about a function call?
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%          Lecture 3: C Pointers, Arrays, Memory Management
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture[06/22/23]{C Pointers, Arrays, Memory Management}

% TODO DELETE Section
\lecture[06/27/23]{C Memory}
Stack grows down, i.e, starts af FF and decreases.

The stack memory is destroyed as you return from functions

The heap memory does not, you are responsible for destorying (freeing) the memory when you don't need it anymore.


\section*{Chapter 5: Pointers and Arrays}
\begin{definition}[Pointer]
    A pointer is a variable that contains the address of another variable. The convention to naming pointer variables is the prefix them with \lstinline|p_<name>|.

    An pointer that points to an \lstinline|int| is declared as follows:

    \lstinline{int *p_x;}

    It says that the combination of \lstinline|*p_x| is an \lstinline|int|, that is, if the derefencing operator is used on \lstinline|p_x|, it is equivalent to a variable of type \lstinline|int|.
\end{definition}


\begin{itemize}
    \item The unary operator \lstinline|&| gives the \emph{address} of an object.
    \item The dereference operator \lstinline|*| used in the context as an operand to an address, access that address to fetch the contents to which it points to.
\end{itemize}






\subsection*{Pointers and Syntax}
\begin{itemize}
    \item pointers can occur on the left side of assignments. That is \lstinline{p_x} points to \lstinline{x}, then \\ \lstinline{*p_x = 1} sets \lstinline{x} to 1.
    \item Normally a pointer can point to only one type, however, \lstinline{void *} is a type that can point to anything. (use sparingly)
\end{itemize}

\subsection*{Arrays}
Arrays and pointers have a strong relationship. The name of an array is actually an address location to the zeroth element in the collection. \newline
So saying \lstinline{p_arr = arr} is equivalent to saying \lstinline{p_arr = &arr[0]}

Moreover, indexing into an array is actually using pointer arithmetic under the hood. For example: doing \lstinline{arr[3]} is equivalent to \lstinline{*(a + 3)}.

\subsubsection*{Address Arithmetic}
The only difference between an array name and a pointer is that a pointer is a variable, but an array name is a \textbf{constant}. Constructions like \lstinline{a = p_arr} or \lstinline{a++} or \lstinline{p = &a} are illegal.


\subsubsection*{Memory Organization}
Memory is just a large, single-dimensional array with the, that is byte-addressable, where the address acting as the index to that array. 8 bits = 1 byte, and 4 bytes = 1 word.



\textbf{Type Sizes:} Assume we are working with a 32-bit system.
\begin{itemize}
    \item \lstinline{int} occupy 4 bytes, aka 32 bits.
    \item \lstinline{char} occupy 1 byte
    \item \emph{pointers} occupy 4 bytes. This is because memory address are 32 bits long in a 32-bit system (why?), therefore the pointer values, the address that the pointer points to, are also 32 bits long.
\end{itemize}
\begin{definition}[Endianness Systems]
    Endianness affects how a group of bytes are stored and read in memory.

    \textbf{Little-Endian:} In a little-endian system, memory is stored with the most significant byte at the height address.

    \textbf{Big-Endian:} In a big-endian system, memory is stored with the most significant byte at the lowest address.
\end{definition}

\begin{example}
    For the program below assume, the memory is drawn as such in little endian system.

    \begin{lstlisting}
        int main() {
            int x[2]; // address of x is: 0x1000 0000
            x[0] = -2, x[1] = 44513;
            char y[] = "ADEL"; // address of y is: 0x1000 0010
            char *c y;
        }
    \end{lstlisting}

    The corresponding memory model is as follows

    \begin{center}
        \begin{tabular}{|c|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{2cm}|}
            \hline
            \textbf{Address} & \textbf{Byte 0} & \textbf{Byte 1} & \textbf{Byte 2} & \textbf{Byte 3} \\
            \hline
            0x1000 0000      & 0xFE            & 0xFF            & 0xFF            & 0xFF            \\
            \hline
            0x1000 0004      & 0xE1            & 0xAD            & 0x00            & 0x00            \\
            \hline
            0x1000 0008      & -               & -               & -               & -               \\
            \hline
            0x1000 000C      & -               & -               & -               & -               \\
            \hline
            0x1000 0010      & 0x41            & 0x44            & 0x45            & 0x4C            \\
            \hline
            0x1000 0014      & 0x00            & -               & -               & -               \\
            \hline
            0x1000 0018      & 0x10            & 0x00            & 0x00            & 0x10            \\
            \hline
        \end{tabular}
    \end{center}

    \textbf{Note:} While \lstinline{ints} get stored in reverse, character arrays or strings are stored in increasing memory addresses.


\end{example}


\begin{itemize}
    \item type declaration tells compiler how many bytes to fetch on each access through pointer.
    \item
\end{itemize}

\subsubsection*{Dynamic Allocation}
\begin{itemize}
    \item malloc: memory Allocation
    \item calloc: cleared allocation
    \item realloc: re-allocation
\end{itemize}


68 6c 70 74 78 7c 80 84 88 8c 90





\section*{Questions}
\begin{itemize}
    \item is there double pointers, a pointer that points to another pointer? \textbf{Yes, you write a function to increment a pointer. Here it will accept a double point as an argument}
    \item Is there null in C? Can you initialize a pointer to be null? \textbf{Yes, C has a null keyword, NULL}
    \item It is said that the difference between arrays and pointers is that array names is a constant, not a variable. Can you have a pointer to a constant? Why is \lstinline{p = &arr} illegal?

\end{itemize}



\lecture[06/27/23]{Floating Point}

Just as we can show decimal numbers in scientific notation, we can also show binary numbers in scientific notation:
$$
    1.0_{\text{two}} \times 2^{-1}
$$

To keep a number in normalized form, we need a base that allows us to shift the binary point left or right to have on nonzero digit to the left of the decimal point. Only base 2 fulfills this since multiplication by 2 is a left shit and division is a right shift.


\begin{definition}[Floating point]
    Computer arithmetic that represents numbers in which the binary point is not fixed, i.e., it floats.
    The floating-point representation is as such:\\

    \begin{center}
        \begin{tabular}{|*{32}{c}}
            \hline
            31                      & 30                            &                                         &  &  &  &  &  &  & 23 & 22 &  &  &  &  &  &  &  &  &  &  &  &  & \multicolumn{1}{c|}{0} \\ \hline

            \multicolumn{1}{|c|}{s} & \multicolumn{9}{c|}{exponent} & \multicolumn{14}{c|}{fraction/mantissa}                                                                                          \\ \hline
        \end{tabular}
    \end{center}

    The scientific notation in binary form of the float is a singly nonzero digit to the left of the binary point (\textbf{normalized form}) as such:

    $$
        1.\underbrace{xxxxxxxxx_{\text{two}}}_{\color{orange}{\text{mantissa/significand}}} \times 2^{\text{yyyy} \rightarrow \color{blue}{\text{exponent}}}
    $$
    Where $1.xxxxxxxxx$ is the significand ($0 < S < 1$), and $yyyy$ is called the exponent.

    To pack more numbers in the significand, IEEE 754 makes the leading 1-bit of a normalized binary numbers implicit. So now, the mantissa is $1 + \text{23-bit significand field}$. Therefore, the simple RISC-V floating number as such:

    $$
        (-1)^S \times (1 + F) \times 2^E
    $$
    Where $S$ is the sign of the floating-point number, $F$ involves the value in the fraction field, generally a number between 0 and 1, also called the \emph{mantissa}. And $E$ comes from the exponent field.

    Note however, float comparisons are hard with this form. The most negative exponent is all 1s, and $1_\text{ten}$ is a single 1 bit in the least significant bit. The negative number looks larger. Therefore we introduce bias notation to shift the numbers where now the most negative number is $00 \dots 00_{\text{two}}$ and the most positive as $11 \dots 11_{\text{two}}$. IEEE 754 uses a bias of 127 for single precision, and 1023 for double precision.

    Biased exponent means that the value represented by a floating-point number is really:


    $$
        (-1)^S \times (1 + F) \times 2^{E - \text{Bias}}
    $$
\end{definition}


\begin{example}
    What is the decimal equivalent of the following IEE 754 single-precision binary floating point number?
    \begin{center}
        \begin{tabular}{|*{32}{c}}
            \hline
            31                      & 30                             &                                         &  &  &  &  &  &  & 23 & 22 &  &  &  &  &  &  &  &  &  &  &  &  & \multicolumn{1}{c|}{0} \\ \hline

            \multicolumn{1}{|c|}{1} & \multicolumn{9}{c|}{1000 0001} & \multicolumn{14}{c|}{111 0000 ... 0000}                                                                                          \\ \hline
        \end{tabular}
    \end{center}

    \begin{align}
         & = (-1)^1 \times 1.111 \times 2^{129 - 127}                                          \\
         & = -1 \times 1.111 \times 2^{2}                                                      \\
         & = -1 \times 111.1                          &  & \text{shift the decimal point by 2} \\
         & = -1 \times (4 + 2 + 1 + \frac{1}{2})                                               \\
         & =  -7.5
    \end{align}
\end{example}

\subsection*{Step Size}
Because we have a fixed \# of bits, we cannot represent all numbers. \textbf{Step size} is the spacing between consecutive floats with a given exponent.

What we really are asking for is what is the next representable number after y? before y?

The next step size to why is just adding y to the smallest bit in the significand times the same exponent.
$$
    y + ((0.0\dots001) \times 2^{(E - Bias)})
$$

Note we multiply by the same exponent because y decimal was shifted, so we also need to shift the smallest bit decimal to the same position.

\emph{Note:} the bigger the exponent, the bigger the step size since it's shifted more.
And the smaller the exponent, the smaller the step size


\subsection*{Representing Zero}
Note: Zero has no normalized representation (there is always an implicit 1 in the significand)



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%          Lecture 6: Risc-V Intro
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture[06/28/23]{Risc-V Intro}The operands of arithmetic instructions are restricted, in that they must be from a limited number of special locations built directly in hardware called \emph{registers}.
\begin{definition}[Registers]
    Registers are primitives used in hardware design, often thought of as the "bricks of computer construction". The size of a register in RISC-V architecture is 32 bits, aka 1 word.

    There is a limited number of registers, 32 registers. The reason for the limit of 32 registers is because of the underlying design principles of hardware; \textbf{smaller is faster.} Accessing registers takes less time and uses much less energy than accessing memory.


\end{definition}

In code, we usually have more complex data structures such as arrays. These data structures usually contain more data elements than there are registers. How can a computer represent and access such large structures? They are kept in memory and accessed via instructions that transfer data between memory and registers. Such instructions are called \textbf{data transfer instructions.}

It is the computer job to associate program variables with registers. Usually a program contains many more variables than computer registers. The compiler tries to keep the most frequently used variables in registers and places the rest in memory, data transfer instructions to between registers and memory. This process is called \emph{spilling registers}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%          Lecture 7: RISC-V Procedures
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture[06/29/23]{RISC-V Procedures}
\subsection*{Calling Convention}

Saved Registers:
\begin{itemize}
    \item s0-s11 ra
    \item should not be modified by functions (can be used, but must be restored).
\end{itemize}

Temporary Registers:
\begin{itemize}
    \item {\color{red} TODO}
\end{itemize}


Stack


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%          Lecture 8: RISC-V Instruction Format
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture[07/05/23]{RISC-V Instruction Format}
Instructions are kept in the computer as a series of high and low (1s and 0s) electronic signals and may be represented as numbers. Placing these numbers side by side forms the instruction. The numeric version of instructions is called \textbf{machine language}, and a sequence of such instructions is called \emph{machine code}.

Instructions in the numeric version are split up into \textbf{fields}, which are given names to make them easier to discuss. Here is the meaning of each name of the fields in RISC-V instructions:
\begin{itemize}
    \item \emph{\textbf{opcode:}} Instruction identifier. The field that denotes the operation and format of an instruction, and is always the last 7 bits in all instruction formats.
    \item \emph{rd:} The register destination operand. It gets the result of the operation.
    \item \emph{funct3:} An additional opcode field.
    \item \emph{rs1:} The first register source operand.
    \item \emph{rs2:} The second register source operand.
    \item \emph{funct7:} An additional opcode field.
\end{itemize}

Different instructions require different fields sizes. For example, \lstinline{add} requires 3 registers, \lstinline{addi} requires 2 registers and 1 immediate. If rs2 took the value of the immediate, the maximum size it could hold is $2^5 - 1$ (31 which is pretty small) since rs2 is 5 bits long. Since RISC-V designers decided to keep all instructions the same length (32 bits), this requires distinct instruction formats for different kinds of instructions.

The formats are distinguished by the values in the opcode field (check reference sheet to see the different types of formats).

\subsection*{R-Type}
Designed for instructions with 3 registers and no immediate.

Since each register is identified by it's number, we'd need 5 bits to encode 32 different values ($2^5 = 32$).


\subsection*{I-Type}
Designed for instructions with 2 registers (rs1 and rd) and 1 immediate.
Some instructions that use I-Types include arithmetic operations with immedates and load instruction.

In the R-Type, there is no funct7 field, instead, that space is used by the immediate the be able to get larger constants (12 bits long immediate)

The immediate field is in unsigned representation, so the range of values is [-2048, 2047] [$-2^{11}, 2^{11} - 1$]

\subsection*{I*-Type}
Used for shifting with immediates (slli, srli, srai) where the immediate field is reduced to 5 bits long. This is because in shifting a 32-bit number, you never need to shift more than 32 times.

\subsection*{S-Type}
S for \textbf{store instructions}. Designed for instructions with 2 source registers and an immediate.


\subsection*{U-Type}
Used for \lstinline{lui} (\emph{load upper immediate}) and \lstinline{auipc} (\emph{add upper immediate to program counter}) instructions.

These two instructions are used in two pseudoinstructions:
\begin{itemize}
    \item \lstinline{li}: load immediate
    \item \lstinline{la}: load address
\end{itemize}

\begin{example}[\lstinline{li} with Large Immediate]
    consider this instruction:
    \begin{lstlisting}
        li t0 0x12345678
    \end{lstlisting}
    We cannot just translate this instruction to: \lstinline{addi t0 x0 0x12345678} since the immediate size of \lstinline{addi} is 5 bits long. Assuming that lui has an immediate size of 20 bits (which it does), the solution then is to do this:

    \begin{lstlisting}
        lui to 0x12345 // 20 bit long immediate (5 * 4)
        addi t0 t0 0x678 // 12 bit immediate (fits in addi immediate field.)
    \end{lstlisting}
    * \emph{Note there is a corner case when the immediate in an li instructions has FFF in the 3 right most bits, since addi with an immediate of FFF subtracts $-1$. }
\end{example}

\subsection*{B-Type}
B for branch instructions. Recall, labels don't actually exist. When translating RISC-V to machine language, we need to convert all labels into explicit references to a particular line of code. Since we want to be able to move around code blocks in memory, we prefer to use \textbf{\emph{relative addressing}} instead of absolute addresses.

Therefore, when writing code using labels, we first convert the label into an \emph{offset}, which specifies how many bytes off from the current location (\lstinline{PC} - program counter) we would need to jump to get to that label.

\begin{example}[Converting Labels to Offsets]
    Consider the following code
    \begin{lstlisting}
        beq x0, x0, target // + 2 instructions = 8 bytes, so offset = 8
        addi x0, x0, 100
        target: addi x0, x0, 100 
        j target           // - 1 instruction = -4 bytes, so offset = -4
        li t0 0x5F3759DF
        beq t0, t0, target // -4 instructions = -16 bytes, so offset = -16
    \end{lstlisting}
    \emph{* note li is a pseudoinstructions for 2 instructions}
\end{example}

\subsubsection*{Storing Offsets:}
Note all of the previous offsets were multiples of 4 since each instruction is 4 bytes of memory, so all offsets should be multiples of 4. Therefore, the last two bits off the offset are always going to be 0s. For optimization purposes, we don't store the lowest bit of an offset immediate, brining forth the assuming that the LSB an offset is 0.

In the instruction format, the immediate representing the offset is stored in a strange pattern:
\lstinline{imm[12 | 10:5]}  for bits 31 through 25 of the B-Type format, and \lstinline{imm[4:1 | 11]} for bits 11 thorough 7.
\begin{itemize}
    \item if we have the binary $\text{0b A BCDE FGHI JKLM}$ (where each letter was a bit), the first immediate field would store $\text{0b ACD EFGH}$.
    \item the second immediate field would store $\text{0b I JKLB}$. \emph{note bit M isn't stored}
\end{itemize}



\subsection*{J-Type}
J for jump instruction, \lstinline{jal}. This instruction uses only 1 destination register and an immediate example: \lstinline{jal ra label}, so we can use the U-type format to have a larger immediate, but the immediate is stored in a strange format, to simplify the underlying circuit.

The immediate is stored as: \lstinline{imm[20 | 10:1 | 11 | 19:2]}

For example: If we had the binary $\text{0xA BCDE FGHI JKLM NOPQ RSTU}$ (where each letter was a bit), the data would be stored as 0b $\text{AKLM NOPQ RSTJ BCDE FGHI}$



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%          LECTURE 9: Compiler, Assembler, Linker, Loader
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture[07/06/23]{Compiler, Assembler, Linker, Loader: CALL}
\subsection*{Compiler}
The compiler transforms the C program into an \textbf{assembly language} program, a symbolic form of what the machine understands.

\begin{definition}[Assembly Language]
    a symbolic language that can be translated into binary machine language.
\end{definition}

\subsection*{Assembler}
The assembler converts \textbf{pseudoinstructions} into the machine language equivalent of the actual assembly language instruction. It also converts branches to faraway locations into a branch and a jump.
\begin{definition}[Pseudoinstructions]
    A common variation of assembly language instructions often treated as if it were an instruction in its own right.
\end{definition}
The primary task of an assembler is assembly into machine code. The assembler turns the assembly language program into an \emph{object file}, which is a combination of machine language instructions, data, and information needed to place instructions properly in memory.

To Produce binary version of each instruction into assembly language program, the assembler must determine the address corresponding to all labels. Assemblers keep track of labels used in branches and data transfer instruction in a \textbf{symbol table}.

\subsection*{Linker}
The linker produces an \textbf{executable file} that can be run on a computer.
The linker resolves all references, resolving all undefined labels, patching both internal and external references.

\subsection*{Loader}
A systems program that places an object programs n main memory is that it is ready to execute.


\lecture[07/10/23]{Combinational Logic, FSM}

\section*{A.2: Gates, and Logic Equations}
Blocks without memory are called \emph{combinational}; the output of a combinational block depends only on the current input.

In blocks with memory, the output can depend on both the current input and the value stored in memory, which is called the \emph{state} of the logic block.

\begin{definition}[Combinational Logic]
    A logic system whose blocks do not contain memory and hence compute the same output given the same input.
\end{definition}

Since combinational logic block contains no memory, it can be completely specified by defining the values of the outputs for each possible set of input values. Such a description is normally given as a \emph{truth table.} Truth tables can completely describe any combinational logic function; however; they grow in size quickly ($2^n$ entries for $n$ inputs) making them hard to understand.

\subsection*{Boolean Algebra}
Used to represent signals in logic circuits. One alternative to truth tables is to express the logic function with logic equations. In boolean algebra, all the variables have the values 0 or 1, and in typical formulations, there are three operators:

\begin{itemize}
    \item OR operator, written as $A + B$.
    \item AND operator, written as $A \cdot B$.
    \item unary NOT operator, written as $\overline{A}$
\end{itemize}

Any set of logic functions can be written as a series of equations with an output on the left-hand side of each equation and a formula consisting of variables and the three operators above on the right-hand side.

\subsection*{Gates}
A device that implements basic logic functions, such as AND or OR.

Since both AND and OR are commutative and associative, they can take in multiple inputs, with the output equal to the AND or OR of all the inputs.

Fact: all logic functions can be constructed with only a single gate type, if that gate is inverting. NOR and NAND gates are called \emph{universal}, since any logic function can be built using this one gate type.

\section*{A.3: Combinational Logic}
 {\color{red} TODO}


\section*{FSM: Finite State Machines}
 {\color{red} TODO}

\subsubsection*{Questions}
\begin{itemize}
    \item is the memory contained inside the logic block called the state? or is the output called the state?
    \item
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%          Lecture 11: Synchronous Digital Systems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lecture[07/11/23]{Synchronous Digital Systems}
Synchronous Digital Systems are made up of two basic types of circuits. Combinational Logic circuits, and state elements. \textbf{Combinational logic} circuits change based on their inputs after whatever propagation delay is associated with them. Their like a function.

\section*{State Elements}
\textbf{State elements,} on the other hand can \emph{remember} their inputs even after the inputs change. State elements change value based on a \emph{clock signal.}
\subsubsection*{Clock:}
\begin{itemize}
    \item Rising Edge: Time when the clock switches from 0 to 1
    \item Falling Edge: Time when the clock switches from 1 to 0.
    \item Clock Period: Time between rising edges.
    \item Clock Frequency: Number of rising edges per second.
\end{itemize}

Like logic gates, registers also have a delay associated with them before their output will reflect the input that was sampled at the rising edge of the clock. This is called \textbf{clk-to-q} delay ("Q" often indicates the output.) This is the time between the rising edge of the clock signal and the time the register's output reflect the input change.

\subsection*{Flip Flop}
A Flip Flop is a logic element that 'stores state'.
On the rising edge of the clock, the input is sampled and transferred to the output. At all other times, the input is ignored.

For circuits with multiple flip flops, the flip flop respond to the input at a single time instant when the click is at a rising edge.

\subsubsection*{Time Constraints}
Flip flop sometimes have what's called time constraints. ($t_{\text{setup}}$, $t_{\text{hold}}$) These are \textbf{not} delays, but referred to as constraints because they must be followed inorder for the circuit to work properly. Below is the definition of the timing constraints:

\textbf{setup time}: $t_{\text{setup}}$
\begin{itemize}
    \item This is the amount of time that the flip flop input needs to be stable \emph{\textbf{BEFORE}} the positive edge of the clock.
\end{itemize}


\textbf{hold time}: $t_{\text{hold}}$
\begin{itemize}
    \item the minimum amount of time that the flip-flop input needs to be stable \emph{\textbf{AFTER}} the positive edge of the clock.
\end{itemize}

\subsubsection*{Satisfying setup time constraint (minimum clock period)}
Note during a single clock period, there is a clk-to-q time delay, and some logic delay (assuming there is logic elements in the circuit), and finally a setup time constraint for the next rising edge. Therefore, we need to have a clock period to be long enough so that it could hold all of these timing constraints and delays.

$$t_{\text{clk-to-q}} + t_{\text{logic max}} + t_{\text{setup}} \le t_{\text{clk-period}}$$

\subsubsection*{Satisfying hold time constraint}
It is easy to see that for a circuit without a logic elements, the hold time for a flipflop should always be less than or equal to the clk-to-q.

However, when circuits contain logic elements, which have delays of their own, we'd need to hold time to be short enough so that it is less than the sum of the clk-to-q and the shortest logic delay path. Therefore, the inequality needed so that we don't violate the hold time constraint is as such:

$$
    t_{\text{clk-to-q}} + t_{\text{logic min}} \ge t_{\text{hold}}
$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%          Lecture 12: Single Cycle Datapath
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\lecture[07/12/23]{RISC-V Single Cycle Datapath}

For every implementation of instructions; the first two steps are identical:
\begin{enumerate}
    \item Send the \emph{program counter} (PC) to the memory that contains the code and fetch the instruction from that memory.
    \item Read one or two registers, using fields of the instruction to select the registers to read. For the \lstinline{lw} instruction, we need to read only one register, but most other instructions require reading two registers.
\end{enumerate}

After these two steps, the actions required to complete the instruction depend on the instruction class (memory-reference, arithmetic-logical, and branches)

\begin{definition}

\end{definition}






\lecture[07/17/23]{Single Cycle Datapath Controls, Intro to Pipelining}
\end{document}





